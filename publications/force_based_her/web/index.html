<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0042)https://richardrl.github.io/relational-rl/ -->
<html>
<!-- template from Deepak Pathak https://people.eecs.berkeley.edu/~pathak/ -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <script type="text/javascript" async="" src="./analytics.js"></script>
  <script type="text/javascript" id="www-widgetapi-script" src="./www-widgetapi.js" async=""></script>
  <script src="./jsapi" type="text/javascript"></script>
  <script src="/js/mathjax-config.js"></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/latex.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/tex.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full"
    integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>



  <script type="text/javascript">
    google.load("jquery", "1.3.2");
  </script>
  <style type="text/css">
    body {
      font-family: "Titillium Web", "HelveticaNeue-Light",
        "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial,
        "Lucida Grande", sans-serif;
      font-weight: 300;
      font-size: 18px;
      margin-left: auto;
      margin-right: auto;
      width: 1100px;
    }

    h1 {
      font-weight: 300;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.rounded {
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
      color: #1367a7;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    .layered-paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }

    .layered-paper {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
      /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }

    .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
    }

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right,
          rgba(0, 0, 0, 0),
          rgba(0, 0, 0, 0.75),
          rgba(0, 0, 0, 0));
    }

    #authors td {
      padding-bottom: 5px;
      padding-top: 30px;
    }
  </style>

  <script type="text/javascript" src="./hidebib.js"></script>
  <link href="./css" rel="stylesheet" type="text/css" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="https://richardrl.github.io/relational-rl/resources/seal_icon.png" />
  <title>Force-Based Hindsight Experience Prioritization</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://richardrl.github.io/relational-rl/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Relational Reinforcement Learning" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Force-Based Hindsight Experience Prioritization" />
  <meta property="og:description"
    content="Li, Jabri, Darrell, Agrawal. Force-Based Hindsight Experience Prioritization. 2019." />
  <meta property="og:url" content="https://richardrl.github.io/relational-rl/" />
  <meta property="og:image" content="https://pathak22.github.io/modular-assemblies/resources/generalization.png" />
  <meta property="og:video" content="https://www.youtube.com/embed/rW1HTX0VA10" />

  <meta property="article:publisher" content="https://github.com/richardrl/richardrl.github.io/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Force-Based Hindsight Experience Prioritization" />
  <meta name="twitter:description"
    content="Li, Jabri, Darrell, Agrawal. Force-Based Hindsight Experience Prioritization. 2019." />
  <meta name="twitter:url" content="https://richardrl.github.io/relational-rl/" />
  <meta name="twitter:image" content="https://richardrl.github.io/relational-rl/resources/teaser.jpg" />
  <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Richard Li" />
  <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" />
  <meta name="twitter:site" content="@richardli" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="./iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://richardrl.github.io/relational-rl/resources/teaser.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/rW1HTX0VA10" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "UA-89486716-2");
  </script>
  <!-- 
<script>
var player, seconds = 0;
function onYouTubeIframeAPIReady() {
    console.log("player");
    player = new YT.Player('yt-embed', {
        events: {
          'onReady': onPlayerReady
        }
      });
}

function onPlayerReady(event) {
    event.target.playVideo();
}


function seek(sec){
    if(player){
        seconds += sec;
        player.seekTo(seconds, true);
    }
}</script>
 -->
</head>

<body>
  <br />
  <center>
    <span style="font-size: 44px; font-weight: bold">Force-Based Hindsight Experience Prioritization
    </span>
  </center>
  <br />
  <table align="center" width="600px">
    <tbody>
      <tr>
        <td align="center" width="150px">
          <center>
            <span style="font-size: 22px"><a href="https://erdisayar.github.io/" target="_blank">Erdi Sayar</a></span>
          </center>
        </td>
        <td align="center" width="150px">
          <center>
            <span style="font-size: 22px"><a href="https://zhenshan-bing.github.io/" target="_blank">Zhenshan
                Bing</a></span>
          </center>
        </td>
        <td align="center" width="150px">
          <center>
            <span style="font-size: 22px"><a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"
                target="_blank">Alois Knoll</a></span>
          </center>
        </td>

      </tr>
      <tr></tr>
      <tr>
        <td align="center" width="200px">
          <center>
            <span style="font-size: 20px">Technical University of Munich</span>
          </center>
        </td>
        <td align="center" width="200px">
          <center><span style="font-size: 20px">Technical University of Munich</span></center>
        </td>
        <td align="center" width="200px">
          <center><span style="font-size: 20px">Technical University of Munich</span></center>
        </td>
      </tr>
      <tr></tr>
    </tbody>
  </table>

  <table align="center" width="700px">
    <tbody>
      <tr>
        <td align="center" width="700px">
          <center><span style="font-size: 22px">IROS 2023</span></center>
        </td>
      </tr>
      <tr></tr>
    </tbody>
  </table>

  <table align="center" width="700px">
    <tbody>
      <tr>
        <td align="center" width="200px">
          <center>
            <span style="font-size: 22px"><a href="https://arxiv.org/pdf/paper_pdf_adress.pdf">[Download
                Paper]</a></span>
          </center>
        </td>
        <td align="center" width="200px">
          <center>
            <span style="font-size: 22px"><a href="https://github.com/erdisayar/">[GitHub Code]</a></span>
          </center>
        </td>
      </tr>
      <tr></tr>
    </tbody>
  </table>
  <br />

  <table align="center" width="300px">
    <tbody>
      <tr>
        <td align="center" width="300px">
          <iframe width="768" height="512" src="./video.html" frameborder="0" allowfullscreen=""></iframe>
        </td>
      </tr>
    </tbody>
  </table>
  <!--       <br>
 -->
  <div style="width: 800px; margin: 0 auto">
    <br />
    Sections
    <ul style="margin-top: 0px">
      <li> <b> Abstract </b> </li>
      <li> <b>Methodology </b></li>
      <ul style="margin-top: 0px">
        <li>Force-Based Replay Buffer</li>
        <li>Work-Based Replay Buffer</li>
      </ul>
      <li><b> Experiment </b> </li>
    </ul>
    <center>
      <h3>Abstract</h3>
    </center>
    Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due
    to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay
    (HER) expedites learning by taking advantage of failed trajectories and replaces the desired goal with one of the
    achieve points so that any failed trajectory can be utilized as a contribution to learning. But in HER, failed
    trajectories are chosen uniformly, without taking into account which ones might be the most valuable for learning.
    In this paper, we address the sample inefficiency of HER and propose methods for prioritizing the replay buffers
    such as force-based and work-based replay buffers, using force sensors in the gripper of the robot. Contact rich
    experiences are sampled with higher probability from the replay buffer, which are mostly result from interaction
    between manipulator and object. We showed that our methods surpass state-of-the-art methods on robot manipulation
    tasks.

    <center>
      <h3>Methodology</h3>
    </center>
    When we train the robots to perform manipulation tasks, the sparse reward signal causes insufficient successful
    experiences. HER leverages failed experiences by replacing desired goal with the achieved goal of the failed
    experiences. With this modification, any failed experience return nonnegative reward. However, if the initial
    position of the robot is far from the desired position, hindsight goals generated by HER may not be effective in
    solving the task because they are sampled randomly from the visited states, which are mostly distributed around the
    initial state. Consequently, experiences are not regarded based on their importance for learning, which leads to
    sample inefficiency.
    Instead, we leverage the force sensor and prioritize the experiences using the metrics in the Eq.
    ($\ref{eq:p_episode_based_on_force}$) and Eq. ($\ref{eq:p_episode_based_on_work}$) which are proposed as force and
    work
    based replay buffer, respectively.
    <h3>A. Force Based Replay Buffer</h3>

    Force sensors are used on the gripper's left and right sides to measure net applied forces such as coriolis,
    centripetal, gravitational and contact forces. Gravitational forces are compensated by adding the weight of the
    gripper (i.e., $m g$). Next, measured multi-dimensional force is turned into a scalar value by adding the absolute
    value of each dimension. The scalar value is then cumulatively added through time steps for each episode, as written
    in Eq.


    $$
    \begin{equation}
    f(e,t) = \sum_{j=0}^{n} \sum_{i=0}^{t} \lvert {f}_{left}^{\left(j\right)}\left(e,i\right)\rvert +\lvert {f}_{
    right}^{\left(j\right)}\left(e,i\right)\rvert
    \label{eq:cumulative_force}
    \tag{1}
    \end{equation}
    $$
    where $n$ is the dimension of the force sensor, $t$ is timesteps, $f_{left}$ and $f_{right}$ are the forces from the
    left and right side of the gripper, respectively.


    $$
    \begin{equation}
    p_{episode}(e) = \frac{\sum_{t=0}^{T}f(e,t)}{\sum_{i_{e}=0}^{e} \sum_{t=0}^{T}f(i_{e},t)}
    \label{eq:p_episode_based_on_force}
    \tag{2}
    \end{equation}
    $$

    <h3>B. Work Based Replay Buffer</h3>

    A similar approach described in previous section is used for work-based prioritization. To begin, multidimensional
    force values from the gripper's left and right sides are added together
    with their absolute value until the current timestep $t$. Then, it is multiplied with the displacement vector.

    The probability distribution of the episodes in the Eq. (\ref{eq:p_episode_based_on_work}) is calculated based on
    their work-rich information, and a mini-batch $\mathcal{B}$ is sampled from the replay buffer $\mathcal{R}$
    according to the probability distribution $p_{episode}(e)$.



    $$

    \begin{equation}
    w(e,t) = \sum_{j=0}^{n}\left(\sum_{i=0}^{t} \lvert {f}_{left}^{\left(j\right)}\left(e,i\right)\rvert +\lvert {f}_{
    right}^{\left(j\right)}\left(e,i\right)\rvert \right) \cdot \Delta {x}_i ^{\left(j\right)}
    \label{eq:cumulaive_work}
    \tag{3}
    \end{equation}
    $$
    where $\Delta {x}_i^{\left(j\right)}$ is the displacement vector between current $i$ and previous timestep $i-1$ in
    dimension $j$. $w\left(e,t\right)$ is the cumulative work done until step $t$.
    $$

    \begin{equation}
    p_{episode}(e) = \frac{\sum_{t=0}^{T}w(e,t)}{\sum_{i_{e}=0}^{e} \sum_{t=0}^{T}w(i_{e},t)}
    \label{eq:p_episode_based_on_work}
    \tag{4}
    \end{equation}
    $$

    <center>
      <h3>Experiment</h3>
    </center>
    In this section, we will describe the robot simulation benchmark used for testing the proposed approach.
    <table align="center">
      <tbody>
        <tr>
          <td width="1200px">
            <center>
              <a href="./mujoco_benchmarks.png"><img src="./mujoco_benchmarks.png" height="500px" /></a><br />
            </center>
          </td>
        </tr>
      </tbody>
    </table>

    We tested the proposed approaches using a 7-DOF robotic arm in the MuJoCo simulation and the robotic arm environment
    is designed as a standard benchmark for Multi-Goal RL. Three standard manipulation tasks from the benchmark:
    PickAndPlace, Push, Slide are selected as shown in the figure above. The state is a vector consisting
    of the position, orientation, linear velocity, and angular velocity of all robot joints and objects. A goal
    represents the desired position for an object. It is assumed that the task is accomplished if object reaches the
    goal within the distance set by a threshold. If the object is not in the range of the goal, the agent receives a
    negative reward $-1$; otherwise, the non-negative reward $0$. We tested the proposed approaches with 7 different
    seeds against vanilla HER using these three manipulation tasks and results show that adding force information to the
    state space speeds up the learning effectively.

    <table align="center" width="1000px">
      <tbody>
        <tr>
          <td width="1200px">
            <a href="./algorithm.png"><img src="./algorithm.png" height="400px" /></a><br />
          </td>
        </tr>
      </tbody>
    </table>

    <h4>Pick & Place Task (FetchPickAndPlace-v1)</h4>
    The mission is to grasp the black box with a gripper and place it at the target location shown as a red dot. As the
    robot moves in three dimensions, three-dimensional force values are used; therefore, $n=3$ in
    Eq.(\ref{eq:cumulative_force}) and Eq.(\ref{eq:cumulaive_work}). Both force-based and work-based replay buffers
    outperform the uniform replay buffer, which is used in Vanilla HER. The results are shown in the Fig.
    \ref{fig:result_fetch_pick_and_place}.

    <h4>Push Task (FetchPush-v1)</h4>
    A black box is placed randomly on the table, and the robot's gripper is clamped. The mission is to push the object
    to the target position. We use $n=3$ in Eq.(\ref{eq:cumulative_force}) and Eq.(\ref{eq:cumulaive_work}).

    <h4>Slide Task(FetchSlide-v1)</h4>
    A puck is placed on a long slippery table and the target is outside of the robots workspace. The mission is to hit
    the puck with a specific force so that it slides on the table and stop at the target location. We use $n=2$ in
    Eq.(\ref{eq:cumulative_force}) and Eq.(\ref{eq:cumulaive_work}) because the gripper of the robot mostly does not
    interact with the puck in the z direction.
    <br />
    The simulation results shed light on the performance of the proposed replay buffers.
    The force-based and work-based replay buffers are better than the uniform replay buffer because force values tell us
    more about how the robot and object interact than the uniform replay buffer does. This lets us prioritize useful
    episodes to learn faster.
    The work-based replay buffer is slightly better than the force-based replay buffer in pickandplace and push tasks,
    where the long interactions between robot and object occur compared to the slide environment. The reason of slight
    improvement of work-based replay buffer is that the it considers the displacement of the end effector and the
    force-based not. Imagine that the robot interacts with the object in a small region, i.e., far away from the goal,
    during an episode. In this scenario, the force-based replay buffer might prioritize that episode; however,
    work-based replay does not give as much priority as force-based replay since the work done by the end effector
    results in a smaller value due to its smaller displacement.
    <table align="center" width="1000px">
      <tbody>
        <tr>
          <td width="1200px">
            <a href="./fetch_simulation_success_result.png"><img src="./fetch_simulation_success_result.png"
                height="250px" /></a><br />
          </td>
        </tr>
      </tbody>
    </table>
    <center>
      <h3>Sim2Real</h3>
    </center>
    We tested the proposed force-based and work-based replay buffers in real-life scenarios using Franka robot. To
    begin, we use the Franka robot for three standard manipulation tasks and named them as
    <i>FrankaPickAndPlace-v1</i>
    <i>FrankaPush-v1</i>
    <i>FrankaSlide-v1</i> respectively and <i>FrankaPickAndPlace-v1</i> used for sim2real is shown in the
    Fig.\ref{fig:franka_robot_setups}.

    The experiment was set up with a camera and ArUco markers.
    ArUco markers and the red cube object are detected using the Aruco module and a red filter with the Open-CV library,
    respectively, and the centers of their pixel positions are obtained as shown in the Fig.
    \ref{fig:sim2real_from_camera_view}. Next, the end-effector is moved freely to find out where the ArUco markers are
    with respect to the robot reference frame. Then the cv.getPerspectiveTransform method is used to calculate the
    perspective transform matrix from those four pairs of points. Lastly, the cube's position in relation to the robot's
    reference frame can be found by multiplying the cube's pixel position by this perspective matrix.
    The policy that has been trained is directly transferred from simulation to real life and outputs the linear motion
    in cartesian space relative to its current position as well as the state of the gripper gap. The parameters of
    linear motion are given to the Franka robot using the Frankx library. Due to a hardware issue on the gripper side of
    the robot (i.e., if gripper states are fed into the robot in every timestep, it no longer responds to commands and
    maintains its position), we determined a threshold value to clamp and release the gripper.

  </div>
  <br />
  <hr />


  <center>
    <h1>Zero-shot Generalization</h1>
  </center>
  <div style="width:800px; margin:0 auto; text-align=center">
    We show zero-shot generalization to different block configurations and
    block cardinalities. Our system can stack blocks into taller towers,
    multiple towers and pyramids without additional training. While these
    results are exciting, we acknowledge there is substantial room for
    improving the zero-shot results.
  </div>
  <br />
  <p style="margin-top: 4px"></p>
  <table align="center" width="1000px">
    <tbody>
      <tr>
        <td width="1200px">
          <center>
            <a href="./generalization.png"><img src="./generalization.png" height="500px" /></a><br />
          </center>
        </td>
      </tr>
    </tbody>
  </table>

  <br />
  <hr />

  <center>
    <h1>Emergent Behaviors</h1>
  </center>
  <div style="width:940; height:280; margin:0 auto; text-align=center">
    1.
    <video width="280" height="280" controls="">
      <source src="test_video.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    &nbsp; 2.
    <video width="280" height="280" controls="">
      <source src="resources/videos/pushing.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>

    3.
    <video width="280" height="280" controls="">
      <source src="resources/videos/pick2place2.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    &nbsp;
  </div>

  <div style="width:800px; margin:0 auto; text-align=center">
    <ol>
      <li>
        <b>Singulation:</b> (0:02) In order to not knock over the tower, the
        agent singulates the final black block before picking and placing it.
      </li>

      <li>
        <b>Pushing while grasping:</b> (0:03) The agent performs a
        rolling/pushing behavior on the green block while grasping the blue
        block.
      </li>

      <li>
        <b>Pick 2, place 2:</b> (0:03) The agent collects the blue and yellow
        blocks in hand before placing each one by one.
      </li>
    </ol>
  </div>

  <br />
  <hr />

  <center>
    <h1>Failure Modes</h1>
  </center>
  <div style="width:620; height:580px; margin:0 auto; text-align=center">
    1.
    <video width="280" height="280" controls="">
      <source src="frankapickandplace.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    &nbsp; 2.
    <video width="280" height="280" controls="">
      <source src="frankapush.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>

    3.
    <video width="280" height="280" controls="">
      <source
        src="resources/failure_videos/stack6-&gt;stack6/Blocks Fall Off - Before Stacked/openaigym.video.0.2340.video000043.mp4"
        type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    &nbsp; 4.
    <video width="280" height="280" controls="">
      <source
        src="resources/failure_videos/stack6-&gt;stack6/Blocks Fall Off - After Stacked/openaigym.video.0.2340.video000064.mp4"
        type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

  <div style="width:800px; margin:0 auto; text-align=center">
    <ol>
      <li>
        <b>Oscillation:</b> The agent oscillates its end-effector without
        progressing towards the goal. Often, this happens when the target
        block is very close to the base of the tower. In this scenario,
        picking up the block risks toppling the tower. We hypothesize that in
        order to reduce this risk, the agent simply oscillates its
        end-effector.
      </li>

      <li>
        <b>Insufficient recovery time:</b> After 6 blocks have been stacked
        into a tower, the tower topples. The agent restarts stacking but is
        unable to stack all the blocks within the maximum time length of the
        episode.
      </li>

      <li>
        <b>Blocks fall off during stacking:</b> While stacking a tower of 6
        blocks, the agent knocks one or more blocks off the table. Because the
        blocks are no longer on the table, the agent does not succeed.
      </li>

      <li>
        <b>Blocks fall off after stacking:</b>
        The agent succeeds in stacking a tower of 6 blocks, but the tower
        topples and block(s) fall off the table.
      </li>
    </ol>
  </div>
  <br />
  <hr />

  <center id="sourceCode">
    <h1>Source Code and Environment</h1>
  </center>
  <div style="width:800px; margin:0 auto; text-align=center">
    We have released the PyTorch based implementation and environment on the
    Github page. Try our code!
  </div>
  <table align="center" width="900px">
    <tbody>
      <tr>
        <!-- <p style="margin-top:4px;"></p> -->
        <td width="300px" align="center">
          <span style="font-size: 28px"><a href="https://github.com/erdisayar">[GitHub]</a></span>
        </td>
      </tr>
    </tbody>
  </table>
  <br />
  <hr />

  <h1 style="text-align: center">Paper and Bibtex</h1>
  <table align="center" width="850px">
    <tbody>
      <tr>
        <td width="250px" align="left">
          <!-- <p style="margin-top:4px;"></p> -->
          <a href="https://arxiv.org/pdf/1912.11032.pdf"><img style="height: 150px"
              src="./ICRA_PAPER_RICHARD.pdf.jpg" /></a>
          <span style="font-size: 20pt"><a href="https://arxiv.org/pdf/1912.11032.pdf">[Paper]</a>&nbsp;
            <span style="font-size: 20pt"><a href="https://arxiv.org/pdf/1912.11032.pdf">[ArXiv]</a>
            </span></span>
        </td>
        <td width="50px" align="center"></td>
        <td width="550px" align="left">
          <!-- <p style="margin-top:4px;"></p> -->
          <p style="text-align: left">
            <b><span style="font-size: 20pt">Citation</span></b><br /><span style="font-size: 6px">&nbsp;<br /></span>
            <span style="font-size: 15pt">Richard Li, Allan Jabri, Trevor Darrell, Pulkit Agrawal.
              <b>Towards Practical Multi-object Manipulation using Relational
                Reinforcement Learning.<br /></b>
              In <i>ICRA</i> 2020.</span>
          </p>
          <!-- <p style="margin-top:20px;"></p> -->
          <span style="font-size: 20pt"><a shape="rect" href="javascript:togglebib(&#39;relationalrl19_bib&#39;)"
              class="togglebib">[Bibtex]</a></span>
        </td>
      </tr>
      <tr>
        <td width="250px" align="left"></td>
        <td width="50px" align="center"></td>
        <td width="550px" align="left">
          <div class="paper" id="relationalrl19_bib">
            <pre xml:space="preserve" style="display: none">
@inproceedings{li19relationalrl,
  Author = {Li, Richard and
  Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit},
  Title = {Towards Practical Multi-object Manipulation using Relational Reinforcement Learning},
  Booktitle = {arXiv preprint arXiv:XXXX},
  Year = {2019}
}</pre>
          </div>
        </td>
      </tr>
    </tbody>
  </table>
  <br />
  <hr />

  <!--     <table align=center width=800px>
      <tr><td width=800px><left> -->
  <div style="width: 800px; margin: 0 auto; text-align: left">
    <center>
      <h1>Acknowledgements</h1>
    </center>
    We acknowledge support from US Department of Defense, DARPA's Machine
    Common Sense Grant and the BAIR and BDD industrial consortia. We thank
    Amazon Web Services (AWS) for their generous support in the form of cloud
    credits. We'd like to thank Vitchyr Pong, Kristian Hartikainen, Ashvin
    Nair and other members of the BAIR lab and the Improbable AI lab for
    helpful discussions during this project.
    <a href="https://people.eecs.berkeley.edu/~pathak/">Template credit</a>.
    <!--       </left></td></tr>
    </table> -->
    <br /><br />
    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
  </div>
</body>

</html>