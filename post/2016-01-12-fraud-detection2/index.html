<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.27-DEV" />
  <meta name="author" content="Mathieu Besançon">
  <meta name="description" content="PhD student in applied mathematics for energy systems">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  

  <link rel="alternate" href="https://mbesancon.github.io/index.xml" type="application/rss+xml" title="μβ">
  <link rel="feed" href="https://mbesancon.github.io/index.xml" type="application/rss+xml" title="μβ">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://mbesancon.github.io/post/2016-01-12-fraud-detection2/">

  

  <title>A Pythonic data science project: Part II | μβ</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">μβ</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#experiences">
            
            <span>Experiences</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  <img src="/img/posts/BankNotes/images/svmexample.png" class="article-banner" itemprop="image">
  
</div>



  <div class="article-container">
    <h1 itemprop="name">A Pythonic data science project: Part II</h1>
    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2016-01-12 00:00:00 -0500 EST" itemprop="datePublished">
      Tue, Jan 12, 2016
    </time>
  </span>

  

  
  
  
  <span class="article-tags">
    <i class="fa fa-tags"></i>
    
    <a href="/tags/data-science">data-science</a
    >, 
    
    <a href="/tags/python">python</a
    >, 
    
    <a href="/tags/classification">classification</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmbesancon.github.io%2fpost%2f2016-01-12-fraud-detection2%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=A%20Pythonic%20data%20science%20project%3a%20Part%20II&amp;url=https%3a%2f%2fmbesancon.github.io%2fpost%2f2016-01-12-fraud-detection2%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmbesancon.github.io%2fpost%2f2016-01-12-fraud-detection2%2f&amp;title=A%20Pythonic%20data%20science%20project%3a%20Part%20II"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fmbesancon.github.io%2fpost%2f2016-01-12-fraud-detection2%2f&amp;title=A%20Pythonic%20data%20science%20project%3a%20Part%20II"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=A%20Pythonic%20data%20science%20project%3a%20Part%20II&amp;body=https%3a%2f%2fmbesancon.github.io%2fpost%2f2016-01-12-fraud-detection2%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<p>[1]</p>

<p>Part II: Feature engineering</p>

<hr />

<h1 id="what-is-feature-engineering">What is feature engineering?</h1>

<p>It could be describe as the transformation of raw data to produce
a model input which will have better performance. The <em>features</em> are
the new variables created in the process.
It is often described as based on domain knowledge and more of an
art than of a science. Therefore, it requires a great attention and
a more &ldquo;manual&rdquo; process than the rest of data science projects.</p>

<p>Feature engineering tends to be heavier when raw data are far from
the expected input format of our learning models
(images or text for instance). It can be noticed that some feature
engineering was already performed on our data, since banknotes were
registered as images taken from a digital camera, and we only received
5 features for each image.</p>

<h1 id="correlated-variables">Correlated variables</h1>

<h2 id="simple-linear-and-polynomial-regression">Simple linear and polynomial regression</h2>

<p>We noticed some strong dependencies between variables thanks to the
scatter plot. Those can deter the performance and robustness of
several machine learning models. Skewness and kurtosis seem to be
somehow related. A regression line can be fitted with the skewness as
explanatory variable:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">a, b <span style="color: #555555">=</span> stats<span style="color: #555555">.</span>linregress(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>])[:<span style="color: #FF6600">2</span>]
plt<span style="color: #555555">.</span>plot(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],<span style="color: #CC3300">&#39;g+&#39;</span>)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">2.5</span>,<span style="color: #FF6600">2.5</span>,<span style="color: #FF6600">0.05</span>) ,b<span style="color: #555555">+</span>a<span style="color: #555555">*</span>np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">2.5</span>,<span style="color: #FF6600">2.5</span>,<span style="color: #FF6600">0.05</span>),<span style="color: #CC3300">&#39;r&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Simple linear regression&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Skewness&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Kurtosis&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img src="/img/posts/BankNotes/figures/linear_reg.png" alt="Linear regression" /></p>

<p>The following result highlights a lack in the model. The slope and intercept
seem to be biased by a dense cluster of points with the skewness
between 1 and 2. The points with a low skewness are under-represented in the
model and do not follow the trend of the regression line. A robust regression
technique could correct this bias, but a polynomial regression is the most
straight-forward method to capture a higher part of the variance here.
The second-degree polynomial model can be written as:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">y_hat <span style="color: #555555">=</span> a<span style="color: #555555">*</span>np<span style="color: #555555">.</span>square(x) <span style="color: #555555">+</span> b<span style="color: #555555">*</span>x <span style="color: #555555">+</span> c
</pre></div>


<p>and its coefficients can be determined through the minimization of least-square
error in numpy:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">a, b, c <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>plot(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],<span style="color: #CC3300">&#39;+&#39;</span>)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">15</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>),a<span style="color: #555555">*</span>np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">15</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>) <span style="color: #555555">*</span> np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">15</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)<span style="color: #555555">+</span>b<span style="color: #555555">*</span>np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">15</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)<span style="color: #555555">+</span>c,<span style="color: #CC3300">&#39;r&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;2nd degree polynomial regression&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Skewness&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Kurtosis&#39;</span>)
</pre></div>


<div style ="text-align: center;" markdown="1">
![Polynomial regression](/img/posts/BankNotes/figures/poly_reg.png)
</div>

<p>A polynomial regression yields a much better output with balanced residuals.
The p-value for all coefficients is below the 1% confidence criterion.
One strong drawback can however be noticed: the polynomial model predicts an
increase in the kurtosis for skewness superior to 2, but there is no evidence
for this statement in our data, so the model could lead to stronger errors.</p>

<p>The regression does not capture all the variance (and does not explain all
underlying phenomena) of the Kurtosis, so a transformed variable has to be kept,
which should be independent from the skewness. The most obvious value is the
residual of the polynomial regression we performed.</p>

<p>We can can represent this residual versus the explanatory variable
to be assured that:
* The residuals are centered around 0
* The variance of the residuals is approximately constant with the skewness
* There are still patterns in the Kurtosis: the residuals are not just noise</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">p0 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(d0[<span style="color: #CC3300">&#39;skew&#39;</span>],c<span style="color: #555555">+</span>b<span style="color: #555555">*</span>d0[<span style="color: #CC3300">&quot;skew&quot;</span>] <span style="color: #555555">+</span>a<span style="color: #555555">*</span>d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>d0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;b&#39;</span>,marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;0&quot;</span>)
p0 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(d1[<span style="color: #CC3300">&#39;skew&#39;</span>],c<span style="color: #555555">+</span>b<span style="color: #555555">*</span>d1[<span style="color: #CC3300">&quot;skew&quot;</span>] <span style="color: #555555">+</span>a<span style="color: #555555">*</span>d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>d1[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;r&#39;</span>,marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;1&quot;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Explanatory variable vs Regression residuals&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Skewness&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Residuals&#39;</span>)
plt<span style="color: #555555">.</span>legend([<span style="color: #CC3300">&quot;0&quot;</span>,<span style="color: #CC3300">&quot;1&quot;</span>])
plt<span style="color: #555555">.</span>show()
</pre></div>


<div style="text-align: center;" markdown="1">
![Residuals of the regression](/img/posts/BankNotes/figures/resid_reg.png)
</div>

<p>The data is now much more uncorrelated, so the feature of interest is the
residual of the regression which will replace the kurtosis in the data.</p>

<h2 id="class-dependent-regression">Class-dependent regression</h2>

<p>We can try and repeat the same process for the entropy and skewness, which
also seem to be related to each other.
<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">p0 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(d0[<span style="color: #CC3300">&#39;skew&#39;</span>],c<span style="color: #555555">+</span>b<span style="color: #555555">*</span>d0[<span style="color: #CC3300">&quot;skew&quot;</span>] <span style="color: #555555">+</span>a<span style="color: #555555">*</span>d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>d0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;b&#39;</span>,marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;0&quot;</span>)
p0 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(d1[<span style="color: #CC3300">&#39;skew&#39;</span>],c<span style="color: #555555">+</span>b<span style="color: #555555">*</span>d1[<span style="color: #CC3300">&quot;skew&quot;</span>] <span style="color: #555555">+</span>a<span style="color: #555555">*</span>d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>d1[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;r&#39;</span>,marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;1&quot;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Explanatory variable vs Regression residuals&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Skewness&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Residuals&#39;</span>)
plt<span style="color: #555555">.</span>legend([<span style="color: #CC3300">&quot;0&quot;</span>,<span style="color: #CC3300">&quot;1&quot;</span>])
plt<span style="color: #555555">.</span>show()
plt<span style="color: #555555">.</span>plot(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 0&quot;</span>)
plt<span style="color: #555555">.</span>plot(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;r+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 1&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Skewness&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Entropy&quot;</span>)
plt<span style="color: #555555">.</span>grid()
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>show()
</pre></div>
</p>

<p><img src="/img/posts/BankNotes/figures/skew_entropy.png" alt="Skewness-Entropy" /></p>

<p>We can try can fit a 2nd-degree polynomial function:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">ft <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;entropy&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>plot(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 0&quot;</span>)
plt<span style="color: #555555">.</span>plot(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;r+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 1&quot;</span>)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">14.5</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>),
         ft[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span>np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">14.5</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)<span style="color: #555555">*</span>np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">14.5</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)<span style="color: #555555">+</span>ft[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span>
         np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">14.5</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)<span style="color: #555555">+</span>ft[<span style="color: #FF6600">2</span>],<span style="color: #CC3300">&#39;-&#39;</span>,linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span> ,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Fitted polynom&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Skewness&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Entropy&quot;</span>)
plt<span style="color: #555555">.</span>grid()
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;bottom center&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img src="/img/posts/BankNotes/figures/fit1_entropy.png" alt="Polynomial regression on entropy" /></p>

<p>However, it seems that the model does not fit well our data and that the points
are not equally distributed on both side of the curve. There is another
pattern, which is class-dependent, so two polynomial curves should be fitted,
one for each class:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">f0 <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">15</span>,<span style="color: #FF6600">14</span>,<span style="color: #555555">.</span><span style="color: #FF6600">5</span>)
f1 <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)

plt<span style="color: #555555">.</span>plot(x,f0[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> x<span style="color: #555555">*</span>x<span style="color: #555555">+</span>f0[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> x<span style="color: #555555">+</span>f0[<span style="color: #FF6600">2</span>],<span style="color: #CC3300">&#39;-&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Fitted 0&quot;</span>)
plt<span style="color: #555555">.</span>plot(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;+&#39;</span>,alpha<span style="color: #555555">=.</span><span style="color: #FF6600">7</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 0&quot;</span>)

plt<span style="color: #555555">.</span>plot(x,f1[<span style="color: #FF6600">0</span>] <span style="color: #555555">*</span> x<span style="color: #555555">*</span>x<span style="color: #555555">+</span>f1[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> x<span style="color: #555555">+</span>f1[<span style="color: #FF6600">2</span>],<span style="color: #CC3300">&#39;-&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Fitted 1&quot;</span>)
plt<span style="color: #555555">.</span>plot(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;m+&#39;</span>,alpha<span style="color: #555555">=.</span><span style="color: #FF6600">7</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 1&quot;</span>)

plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Class dependent fit&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Skewness&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Entropy&quot;</span>)
plt<span style="color: #555555">.</span>grid()
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;bottom center&#39;</span>)
plt<span style="color: #555555">.</span>savefig(<span style="color: #CC3300">&quot;class_depend.png&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img src="/img/posts/BankNotes/figures/class_depend.png" alt="Class-dependent polynomial regression" /></p>

<p>The model seems to capture more of the variance in our data, which we can
confirm by plotting the residuals of the class-dependent regression.</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">plt<span style="color: #555555">.</span>plot(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],f0[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">+</span>f0[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> d0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">+</span>
        f0[<span style="color: #FF6600">2</span>]<span style="color: #555555">-</span>d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;b+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 0&quot;</span>)
plt<span style="color: #555555">.</span>plot(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],f1[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">+</span>f1[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> d1[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">+</span>
        f1[<span style="color: #FF6600">2</span>]<span style="color: #555555">-</span>d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],<span style="color: #CC3300">&#39;r+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 1&quot;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>grid()
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Skewness&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Residuals&quot;</span>)
plt<span style="color: #555555">.</span>savefig(<span style="color: #CC3300">&quot;res_class_dep.png&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img src="/img/posts/BankNotes/figures/res_class_dep.png" alt="Residuals of the class-dependent polynomial regression" /></p>

<p>We have a proper working model, with just one problem: <strong>we used
the class to predict the entropy</strong> whereas our classification
objective is to proceed the other way around. Since we noticed
that each class follows a different curve, a difference between
the distance to the first model and the distance to the second
model, which will be noted &ldquo;d&rdquo;, can be computed as:</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">d <span style="color: #555555">=</span> np<span style="color: #555555">.</span>abs(y <span style="color: #555555">-</span> x<span style="color: #555555">.</span>apply(f0)) <span style="color: #555555">-</span> np<span style="color: #555555">.</span>abs(y<span style="color: #555555">-</span>x<span style="color: #555555">.</span>apply(f1))
</pre></div>


<p>A positive &ldquo;d&rdquo; value indicates that the entropy of the observation
is closer to the model fitted on the class 1, this seems to be a
rather relevant indicator to use to build our models. However, this
variable seems correlated to the skewness. The latter could have become
unnecessary for our prediction, so we choose to eliminate it from
the features and take the risk of an information loss.</p>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">d <span style="color: #555555">=</span> <span style="color: #336666">abs</span>(data0[<span style="color: #CC3300">&quot;entropy&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">2</span>])<span style="color: #555555">-</span>\
    <span style="color: #336666">abs</span>(data0[<span style="color: #CC3300">&quot;entropy&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">2</span>])

d0[<span style="color: #CC3300">&quot;d&quot;</span>] <span style="color: #555555">=</span> d[data0[<span style="color: #CC3300">&quot;class&quot;</span>]<span style="color: #555555">==</span><span style="color: #FF6600">0</span>]
d1[<span style="color: #CC3300">&quot;d&quot;</span>] <span style="color: #555555">=</span> d[data0[<span style="color: #CC3300">&quot;class&quot;</span>]<span style="color: #555555">==</span><span style="color: #FF6600">1</span>]

plt<span style="color: #555555">.</span>grid()
plt<span style="color: #555555">.</span>plot(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;d&quot;</span>],<span style="color: #CC3300">&#39;b+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 0&quot;</span>)
plt<span style="color: #555555">.</span>plot(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;d&quot;</span>],<span style="color: #CC3300">&#39;r+&#39;</span>,label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class 1&quot;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;d vs skewness for each class&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Skewness&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;d&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img src="/img/posts/BankNotes/figures/d_skew.png" alt="distance vs skewness for each class" /></p>

<h1 id="variable-scaling">Variable scaling</h1>

<h2 id="common-scaling-techniques">Common scaling techniques</h2>

<p>Very different spreads could be noticed among variables during the exploratory
part. This can lead to a bias in the distance between two points. A possible
solution to this is <strong>scaling</strong> or <strong>standardization</strong>.
* <strong>Variance scaling</strong> of a variable is the division of each value by the
variable standard deviation. The output is a variable with variance 1.</p>

<ul>
<li><strong>Min-Max standardization</strong> of a variable is the division of each value by
the difference between the maximum and minimum values. The outcome values
are all contained in the interval [0,1].</li>
</ul>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">x_stand <span style="color: #555555">=</span> x<span style="color: #555555">/</span>(x<span style="color: #555555">.</span>max()<span style="color: #555555">-</span>x<span style="color: #555555">.</span>min())
</pre></div>


<p>Other standardization operations exist, but those are the
most common because of the properties highlighted.</p>

<h2 id="advantages-and-risks">Advantages and risks</h2>

<p>Scaling variables may avoid the distance between data points
to be over-influenced by high-variance variables, because
the ability to classify the data points from a variable
is usually not proportional to the variable variance.</p>

<p>Furthermore, all people with notions in physics and calculus
would find it awkward to compute a distance from heterogeneous
variables (which would have different units and meaning).</p>

<p>However, scaling might increase the weight of variables carrying mostly
or only noise, to which the model would fit, increasing the error on
new data.</p>

<p>For this case, the second risk seems very low: all variables seem to
carry information, which we could observe because of the low number of
variables.</p>

<h1 id="feature-engineering-pipeline">Feature engineering pipeline</h1>

<div class="highlight" style="background: #f0f3f3"><pre style="line-height: 125%">a, b, c <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(data0[<span style="color: #CC3300">&quot;skew&quot;</span>],data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)

data1 <span style="color: #555555">=</span> data0<span style="color: #555555">.</span>copy() <span style="color: #0099FF; font-style: italic"># copying the data</span>

data1<span style="color: #555555">.</span>columns <span style="color: #555555">=</span> [<span style="color: #CC3300">&#39;vari&#39;</span>, <span style="color: #CC3300">&#39;skew&#39;</span>, <span style="color: #CC3300">&#39;k_resid&#39;</span>, <span style="color: #CC3300">&#39;entropy&#39;</span>, <span style="color: #CC3300">&#39;class&#39;</span>]
data1[<span style="color: #CC3300">&quot;k_resid&quot;</span>] <span style="color: #555555">=</span> data0[<span style="color: #CC3300">&quot;kurtosis&quot;</span>] <span style="color: #555555">-</span> np<span style="color: #555555">.</span>square(a<span style="color: #555555">*</span>(data0[<span style="color: #CC3300">&quot;skew&quot;</span>]) <span style="color: #555555">+</span> b<span style="color: #555555">*</span>data0[<span style="color: #CC3300">&quot;skew&quot;</span>] <span style="color: #555555">+</span> c)

data1<span style="color: #555555">.</span>columns  <span style="color: #555555">=</span> [<span style="color: #CC3300">&#39;vari&#39;</span>, <span style="color: #CC3300">&#39;skew&#39;</span>, <span style="color: #CC3300">&#39;k_resid&#39;</span>, <span style="color: #CC3300">&#39;d&#39;</span>, <span style="color: #CC3300">&#39;class&#39;</span>] <span style="color: #0099FF; font-style: italic"># computing the feature from the entropy regression</span>

f0 <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(d0[<span style="color: #CC3300">&quot;skew&quot;</span>],d0[<span style="color: #CC3300">&quot;entropy&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
f1 <span style="color: #555555">=</span> np<span style="color: #555555">.</span>polyfit(d1[<span style="color: #CC3300">&quot;skew&quot;</span>],d1[<span style="color: #CC3300">&quot;entropy&quot;</span>],deg<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)

data1[<span style="color: #CC3300">&quot;d&quot;</span>] <span style="color: #555555">=</span> <span style="color: #336666">abs</span>(data0[<span style="color: #CC3300">&quot;entropy&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f0[<span style="color: #FF6600">2</span>])<span style="color: #555555">-</span>\
    <span style="color: #336666">abs</span>(data0[<span style="color: #CC3300">&quot;entropy&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">0</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">1</span>]<span style="color: #555555">*</span> data0[<span style="color: #CC3300">&quot;skew&quot;</span>]<span style="color: #555555">-</span>f1[<span style="color: #FF6600">2</span>])

data1 <span style="color: #555555">=</span> data1<span style="color: #555555">.</span>drop(<span style="color: #CC3300">&quot;skew&quot;</span>,<span style="color: #FF6600">1</span>) <span style="color: #0099FF; font-style: italic"># removing skew</span>

data1<span style="color: #555555">.</span>iloc[:,:<span style="color: #FF6600">4</span>] <span style="color: #555555">=</span> data1<span style="color: #555555">.</span>iloc[:,:<span style="color: #FF6600">4</span>]<span style="color: #555555">/</span>np<span style="color: #555555">.</span>sqrt(np<span style="color: #555555">.</span>var(data1<span style="color: #555555">.</span>iloc[:,:<span style="color: #FF6600">4</span>])) <span style="color: #0099FF; font-style: italic"># data normalization</span>
</pre></div>


<p><code>data1</code> can now be used in the next step which will consist in the
implementation of a basic machine learning algorithm. This is the key
part in an analysis-oriented data science project, and I hope to see you there.</p>

<hr />

<p><font size="0.7">
[1] Image source: Philipp Wagner: Machine Learning with OpenCV2
 </font></p>

    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://mbesancon.github.io/post/2016-01-11-fraud-detection/"><span
      aria-hidden="true">&larr;</span> A Pythonic data science project: Part I</a></li>
    

    
    <li class="next"><a href="https://mbesancon.github.io/post/2016-01-13-fraud-detection3/">A Pythonic data science project: Part III <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Mathieu Besançon &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.2/imagesloaded.pkgd.min.js" integrity="sha512-iHzEu7GbSc705hE2skyH6/AlTpOfBmkx7nUqTLGzPYR+C1tRaItbRlJ7hT/D3YQ9SV0fqLKzp4XY9wKulTBGTw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js" integrity="sha512-Z5heTz36xTemt1TbtbfXtTq5lMfYnOkXM2/eWcTTiLU01+Sw4ku1i7vScDc8fWhrP2abz9GQzgKH5NGBLoYlAw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/plugins/ScrollToPlugin.min.js" integrity="sha512-CDeU7pRtkPX6XJtF/gcFWlEwyaX7mcAp5sO3VIu/ylsdR74wEw4wmBpD5yYTrmMAiAboi9thyBUr1vXRPA7t0Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

